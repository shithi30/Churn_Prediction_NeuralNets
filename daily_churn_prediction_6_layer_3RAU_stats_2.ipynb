{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_orig shape: (15, 200000)\n",
      "number of training examples = 170000\n",
      "number of test examples = 30000\n",
      "X_train shape: (15, 170000)\n",
      "Y_train shape: (2, 170000)\n",
      "X_test shape: (15, 30000)\n",
      "Y_test shape: (2, 30000)\n",
      "NaNs exist in training set: False\n",
      "NaNs exist in test set: False\n"
     ]
    }
   ],
   "source": [
    "# the original dataset as numpy matrix \n",
    "XY_orig=genfromtxt('daily_churn_pred_7_2.csv', delimiter=',') \n",
    "XY_orig=np.matrix(XY_orig)   \n",
    "\n",
    "# taking out the X portion\n",
    "X_orig=XY_orig[1:200001, 1:16].T   \n",
    "X_orig_max = X_orig.max(1)          \n",
    "X_orig=X_orig/X_orig_max\n",
    "print (\"X_orig shape: \" + str(X_orig.shape))\n",
    "\n",
    "# taking out the Y portion\n",
    "Y_orig=XY_orig[1:200001, 16].T       \n",
    "\n",
    "# training set\n",
    "X_train=X_orig[:, 0:170000]          \n",
    "Y_train=Y_orig[:, 0:170000]          \n",
    "\n",
    "# test set\n",
    "X_test=X_orig[:, 170000:200000]\n",
    "Y_test=Y_orig[:, 170000:200000]       \n",
    "\n",
    "# convert all examples to numpy arrays\n",
    "X_train=np.array(X_train)   \n",
    "X_train=np.nan_to_num(X_train, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "\n",
    "X_test=np.array(X_test)\n",
    "X_test=np.nan_to_num(X_test, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "\n",
    "Y_train=np.array(Y_train)\n",
    "Y_test=np.array(Y_test)\n",
    "\n",
    "# convert to one-hot representations\n",
    "Y_train=Y_train.astype(int)         \n",
    "Y_test=Y_test.astype(int)\n",
    "Y_train=convert_to_one_hot(Y_train, 2)\n",
    "Y_test=convert_to_one_hot(Y_test, 2)\n",
    "\n",
    "# printing out dimensions\n",
    "print (\"number of training examples = \" + str(X_train.shape[1]))      \n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "\n",
    "# printing out details\n",
    "print(\"NaNs exist in training set: \" + str(np.isnan(X_train).any()))\n",
    "print(\"NaNs exist in test set: \" + str(np.isnan(X_test).any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\progoti\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of real examples = 369598\n",
      "X_real shape: (15, 369598)\n",
      "NaNs exist in training set: False\n"
     ]
    }
   ],
   "source": [
    "# the real dataset as a numpy matrix \n",
    "X_real=genfromtxt('daily_churn_pred_2_full.csv', delimiter=',') \n",
    "X_real=np.matrix(X_real)\n",
    "\n",
    "# taking out the X portion and normalizing the matrix\n",
    "X_real=X_real[1:, 1:16].T   \n",
    "X_real_max=X_real.max(1)          \n",
    "X_real=X_real/X_real_max\n",
    "\n",
    "# convert all examples to numpy arrays for replacing NaNs\n",
    "X_real=np.array(X_real)   \n",
    "X_real=np.nan_to_num(X_real, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "\n",
    "# printing out dimensions\n",
    "print (\"number of real examples = \" + str(X_real.shape[1]))      \n",
    "print (\"X_real shape: \" + str(X_real.shape))\n",
    "\n",
    "# printing out details\n",
    "print(\"NaNs exist in training set: \" + str(np.isnan(X_real).any()))\n",
    "\n",
    "# taking out the mobile numbers \n",
    "df_mobile_nos=pd.read_csv(\"daily_churn_pred_2_full.csv\") \n",
    "mobile_nos=list(df_mobile_nos[\"mobile_no\"])\n",
    "\n",
    "# users with differently predicted retentions\n",
    "pred_0=[]\n",
    "pred_1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [n_x, None])\n",
    "    Y = tf.placeholder(tf.float32, [n_y, None])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \n",
    "    # keeping results consistent\n",
    "    tf.set_random_seed(1)                   \n",
    "        \n",
    "    # defining the neural network's architecture\n",
    "    W1 = tf.get_variable(\"W1\", [20, 15], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [20, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W2 = tf.get_variable(\"W2\", [25, 20], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [25, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W3 = tf.get_variable(\"W3\", [25, 25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [25, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W4 = tf.get_variable(\"W4\", [25, 25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b4 = tf.get_variable(\"b4\", [25, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W5 = tf.get_variable(\"W5\", [20, 25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b5 = tf.get_variable(\"b5\", [20, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W6 = tf.get_variable(\"W6\", [2, 20], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b6 = tf.get_variable(\"b6\", [2, 1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4,\n",
    "                  \"W5\": W5,\n",
    "                  \"b5\": b5,\n",
    "                  \"W6\": W6,\n",
    "                  \"b6\": b6}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    # retrieving the parameters from the dictionary 'parameters' \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    W5 = parameters['W5']\n",
    "    b5 = parameters['b5']\n",
    "    W6 = parameters['W6']\n",
    "    b6 = parameters['b6']\n",
    "                                         \n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                                              \n",
    "    A1 = tf.nn.relu(Z1)   \n",
    "    \n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                                             \n",
    "    A2 = tf.nn.relu(Z2)  \n",
    "    \n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3) \n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    \n",
    "    Z4 = tf.add(tf.matmul(W4, A3), b4)\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    \n",
    "    Z5 = tf.add(tf.matmul(W5, A4), b5)\n",
    "    A5 = tf.nn.relu(Z5)\n",
    "    \n",
    "    Z6 = tf.add(tf.matmul(W6, A5), b6)\n",
    "    \n",
    "    return Z6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z6, Y):\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z6)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.01,\n",
    "          num_epochs = 500, minibatch_size = 64, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # n_x: input size, m : number of examples in the train set\n",
    "    n_y = Y_train.shape[0]                            # n_y: output size\n",
    "    costs = []                                        # to keep track of the cost\n",
    "    \n",
    "    # creating Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # initializing parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # forward propagation: building the forward propagation in the tensorflow graph\n",
    "    Z6 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # cost function: adding cost function to tensorflow graph\n",
    "    cost = compute_cost(Z6, Y)\n",
    "    \n",
    "    # backpropagation: defining the tensorflow AdamOptimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # initializing all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # starting the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # running the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       \n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # selecting a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # printing the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # saving the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        \n",
    "        print (\"Parameters have been trained!\")\n",
    "        print()\n",
    "\n",
    "        # accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(Z6), tf.argmax(Y))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        print()\n",
    "        \n",
    "        # confusion matrix\n",
    "        predicted0_actual0 = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(tf.argmax(Z6), 0), tf.equal(tf.argmax(Y), 0)), \"float\"))\n",
    "        predicted0_actual1 = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(tf.argmax(Z6), 0), tf.equal(tf.argmax(Y), 1)), \"float\"))\n",
    "        predicted1_actual0 = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(tf.argmax(Z6), 1), tf.equal(tf.argmax(Y), 0)), \"float\"))\n",
    "        predicted1_actual1 = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(tf.argmax(Z6), 1), tf.equal(tf.argmax(Y), 1)), \"float\"))\n",
    "\n",
    "        print(\"True negatives:\", predicted0_actual0.eval({X: X_test, Y: Y_test}))\n",
    "        print(\"False negatives:\", predicted0_actual1.eval({X: X_test, Y: Y_test}))\n",
    "        print(\"False positives:\", predicted1_actual0.eval({X: X_test, Y: Y_test}))\n",
    "        print(\"True positives:\", predicted1_actual1.eval({X: X_test, Y: Y_test}))\n",
    "        print()\n",
    "\n",
    "        # precision, recall, f1-score\n",
    "        recall=tf.divide(predicted1_actual1, tf.add_n([predicted1_actual1, predicted0_actual1]))\n",
    "        precision=tf.divide(predicted1_actual1, tf.add_n([predicted1_actual1, predicted1_actual0]))\n",
    "\n",
    "        metric_train_prec=precision.eval({X: X_train, Y: Y_train})\n",
    "        print(\"Train Precision:\", metric_train_prec)\n",
    "        metric_test_prec=precision.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Test Precision:\", metric_test_prec)\n",
    "        \n",
    "        metric_train_rec=recall.eval({X: X_train, Y: Y_train})\n",
    "        print(\"Train Recall:\", metric_train_rec)\n",
    "        metric_test_rec=recall.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Test Recall:\", metric_test_rec)\n",
    "        print()\n",
    "        \n",
    "        metric_train_f1=(2*metric_train_prec*metric_train_rec)/(metric_train_prec+metric_train_rec)\n",
    "        print(\"Train f1:\", metric_train_f1)\n",
    "        metric_test_f1=(2*metric_test_prec*metric_test_rec)/(metric_test_prec+metric_test_rec)\n",
    "        print(\"Test f1:\", metric_test_f1)\n",
    "        print()\n",
    "        \n",
    "        # predictions against test-set\n",
    "        predicted0=tf.equal(tf.argmax(Z6), 0)\n",
    "        predicted1=tf.equal(tf.argmax(Z6), 1)\n",
    "        \n",
    "        predicted_zeros=list(predicted0.eval({X: X_real}))\n",
    "        len_predicted_zeros=len(predicted_zeros)\n",
    "        for i in range(0, len_predicted_zeros):\n",
    "            if(predicted_zeros[i]==True): pred_0.append(mobile_nos[i])\n",
    "                \n",
    "        predicted_ones=list(predicted1.eval({X: X_real}))\n",
    "        len_predicted_ones=len(predicted_ones)\n",
    "        for i in range(0, len_predicted_ones):\n",
    "            if(predicted_ones[i]==True): pred_1.append(mobile_nos[i])\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-7-f26fa0625859>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Cost after epoch 0: 0.639975\n",
      "Cost after epoch 100: 0.575854\n",
      "Cost after epoch 200: 0.571433\n",
      "Cost after epoch 300: 0.564602\n",
      "Cost after epoch 400: 0.563243\n",
      "Parameters have been trained!\n",
      "\n",
      "Train Accuracy: 0.7130706\n",
      "Test Accuracy: 0.7149\n",
      "\n",
      "True negatives: 9435.0\n",
      "False negatives: 2938.0\n",
      "False positives: 5615.0\n",
      "True positives: 12012.0\n",
      "\n",
      "Train Precision: 0.6799035\n",
      "Test Precision: 0.6814546\n",
      "Train Recall: 0.80366653\n",
      "Test Recall: 0.80347824\n",
      "\n",
      "Train f1: 0.736622683610161\n",
      "Test f1: 0.7374527732793704\n",
      "\n",
      "The training took 21.789417759577432 minutes.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "parameters = model(X_train, Y_train, X_test, Y_test)\n",
    "print (\"The training took\", (time.time()-start_time)/60.00, \"minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184949 users predicted to churn.\n",
      "\n",
      "184649 users predicted not to churn.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_1=['0'+str(x) for x in pred_1]\n",
    "print(str(len(pred_1)) + \" users predicted to churn.\\n\")\n",
    "# print(pred_1)\n",
    "\n",
    "pred_0=['0'+str(x) for x in pred_0]\n",
    "print(str(len(pred_0)) + \" users predicted not to churn.\\n\")\n",
    "# print(pred_0)\n",
    "\n",
    "# converting to churn DataFrame to csv\n",
    "dict = {'pred_churn': pred_1} \n",
    "df = pd.DataFrame(dict)\n",
    "df.to_csv(\"pred_churn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL connection is safely closed.\n",
      "Total rows to insert: 184949\n",
      "\n",
      "Progress: \n",
      "50000\n",
      "100000\n",
      "150000\n",
      "184949\n",
      "\n",
      "Elapsed time to write to DB: 5.2605507373809814\n"
     ]
    }
   ],
   "source": [
    "# importing the dataset and specifying where to insert\n",
    "dataset = pd.read_csv('pred_churn.csv')\n",
    "schema='data_vajapora'\n",
    "table='pred_churn'\n",
    "\n",
    "# discarding existing table\n",
    "try:\n",
    "    # connecting to DB\n",
    "    connection=psycopg2.connect(user=\"shithi\", password=\"tallykhata03012021_1234\", host=\"192.168.168.52\", port=\"5432\", database=\"tallykhata\")\n",
    "    cursor=connection.cursor()\n",
    "    \n",
    "    # dropping existing data\n",
    "    postgres_query='''drop table if exists '''+schema+'''.'''+table\n",
    "    cursor.execute(postgres_query)\n",
    "    connection.commit()\n",
    "\n",
    "# handling exception\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error!\", error)\n",
    "    \n",
    "# closing connection to DB\n",
    "finally:\n",
    "    if(connection):\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"PostgreSQL connection is safely closed.\")\n",
    "        \n",
    "# seeing how many rows there are in the DataFrame\n",
    "rows=dataset.shape[0]\n",
    "print(\"Total rows to insert: \"+str(rows))\n",
    "\n",
    "# creating a DataFrame to break the data into chunks\n",
    "df_temp=pd.DataFrame()\n",
    "\n",
    "# counting elapsed time\n",
    "start_time=time.time()\n",
    "\n",
    "# defining how many rows to insert in a chunk\n",
    "step=50000\n",
    "\n",
    "# creating an engine to write data to DB \n",
    "engine=create_engine('postgresql+psycopg2://shithi:tallykhata03012021_1234@192.168.168.52:5432/tallykhata')\n",
    "\n",
    "print()\n",
    "print(\"Progress: \")\n",
    "for i in range(0, rows, step):\n",
    "    \n",
    "    # doing the right increment to the loop variable\n",
    "    if(i+step>rows): step=rows-i\n",
    "    df_temp=dataset[i:i+step]\n",
    "        \n",
    "    # optimizing 'to_sql' method for writing data\n",
    "    df_temp.to_sql(table, engine, schema, if_exists='append', index=False, method='multi')\n",
    "    \n",
    "    # showing progress\n",
    "    print(i+step)\n",
    "\n",
    "# showing how much time it took to do the DB-operation\n",
    "elapsed_time=time.time()-start_time\n",
    "print()\n",
    "print(\"Elapsed time to write to DB: \"+str(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select concat('0', pred_churn) mobile_no from data_vajapora.pred_churn; -- to extract prospective churns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
